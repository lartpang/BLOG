<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Simpler, Better</title><link>https://lartpang.github.io/blog</link><description>Just for fun.</description><copyright>Simpler, Better</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://lartpang.github.io/blog</link></image><lastBuildDate>Wed, 10 Apr 2024 09:46:21 +0000</lastBuildDate><managingEditor>Simpler, Better</managingEditor><ttl>60</ttl><webMaster>Simpler, Better</webMaster><item><title>CVPR 2024 - Open-Vocabulary Video Anomaly Detection</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Open-Vocabulary%20Video%20Anomaly%20Detection.html</link><description># CVPR 2024 - Open-Vocabulary Video Anomaly Detection&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2311.07042&gt;&#13;
&#13;
![image](https://github.com/lartpang/blog/assets/26847524/39e42922-523a-4f5f-b350-23fa5164eeab)&#13;
&#13;
这篇文章主要研究了开放词汇视频异常检测（openvocabulary video anomaly detection，OVVAD）的问题，这是一个具有挑战性但实际重要的问题。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Open-Vocabulary%20Video%20Anomaly%20Detection.html</guid><pubDate>Wed, 10 Apr 2024 09:39:46 +0000</pubDate></item><item><title>CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Retrieval-Augmented%20Open-Vocabulary%20Object%20Detection.html</link><description># CVPR 2024 - Retrieval-Augmented Open-Vocabulary Object Detection&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2404.05687&gt;&#13;
* 代码：&lt;https://github.com/mlvlab/RALF&gt;&#13;
&#13;
本文提出了一种新的开放词汇目标检测方法 Retrieval-Augmented Losses and visual Features (RALF)。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Retrieval-Augmented%20Open-Vocabulary%20Object%20Detection.html</guid><pubDate>Wed, 10 Apr 2024 07:50:42 +0000</pubDate></item><item><title>CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20Interactive%20Image%20Segmentationwith%20Low%20Latency%2C%20High%20Quality%2C%20and%20Diverse%20Prompts.html</link><description># CVPR 2024 - Rethinking Interactive Image Segmentationwith Low Latency, High Quality, and Diverse Prompts&#13;
&#13;
* 论文：&lt;https://arxiv.org/bas/2404.00741&gt;&#13;
* 代码：&lt;https://github.com/uncbiag/SegNext&gt;&#13;
&#13;
![image](https://github.com/lartpang/blog/assets/26847524/70061347-789b-4334-bc13-012d1c713723)&#13;
&#13;
这篇文章主要研究了如何在保持低延迟的同时提高交互式图像分割的质量，并实现多种提示的兼容性。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20Interactive%20Image%20Segmentationwith%20Low%20Latency%2C%20High%20Quality%2C%20and%20Diverse%20Prompts.html</guid><pubDate>Wed, 10 Apr 2024 06:51:40 +0000</pubDate></item><item><title>CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20Inductive%20Biases%20for%20Surface%20Normal%20Estimation.html</link><description># CVPR 2024 - Rethinking Inductive Biases for Surface Normal Estimation&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2403.00712&gt;&#13;
* 代码：&lt;https://github.com/baegwangbin/DSINE&gt;&#13;
&#13;
该研究重新思考了表面法线估计的归纳偏置，提出了利用逐像素射线方向并学习相邻表面法线之间相对旋转关系的方法。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20Inductive%20Biases%20for%20Surface%20Normal%20Estimation.html</guid><pubDate>Wed, 10 Apr 2024 06:27:49 +0000</pubDate></item><item><title>Rethinking Interactive Image Segmentation: Feature Space Annotation</title><link>https://lartpang.github.io/blog/post/Rethinking%20Interactive%20Image%20Segmentation-%20Feature%20Space%20Annotation.html</link><description># Rethinking Interactive Image Segmentation Feature Space Annotation&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2101.04378&gt;&#13;
* 代码：&lt;https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation&gt;&#13;
&#13;
本文提出了一种新的交互式图像分割方法，通过特征空间注释来同时对多张图像进行分割注释，这与现有的交互式分割方法在图像领域进行注解的方式形成了鲜明对比。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Rethinking%20Interactive%20Image%20Segmentation-%20Feature%20Space%20Annotation.html</guid><pubDate>Wed, 10 Apr 2024 06:26:16 +0000</pubDate></item><item><title>CVPR 2024 - Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Efficient%20Deformable%20ConvNets-%20Rethinking%20Dynamic%20and%20Sparse%20Operator%20for%20Vision%20Applications.html</link><description># CVPR 2024 - Efficient Deformable ConvNets - Rethinking Dynamic and Sparse Operator for Vision Applications&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2401.06197&gt;&#13;
* 代码：&lt;https://github.com/OpenGVLab/DCNv4&gt;&#13;
&#13;
本文提出了高效的 DCNv4，这是一个专为视觉应用设计的高效有效的运算符。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Efficient%20Deformable%20ConvNets-%20Rethinking%20Dynamic%20and%20Sparse%20Operator%20for%20Vision%20Applications.html</guid><pubDate>Wed, 10 Apr 2024 06:24:56 +0000</pubDate></item><item><title>CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20the%20Evaluation%20Protocol%20of%20Domain%20Generalization.html</link><description># CVPR 2024 - Rethinking the Evaluation Protocol of Domain Generalization&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2305.15253&gt;&#13;
&#13;
这篇文章主要讨论了领域泛化评估协议的重新思考，特别是如何处理可能存在的测试数据信息泄露风险。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20the%20Evaluation%20Protocol%20of%20Domain%20Generalization.html</guid><pubDate>Wed, 10 Apr 2024 06:23:52 +0000</pubDate></item><item><title>CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection</title><link>https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20the%20Up-Sampling%20Operations%20in%20CNN-based%20Generative%20Network%20for%20Generalizable%20Deepfake%20Detection.html</link><description># CVPR 2024 - Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection&#13;
&#13;
* 论文：&lt;https://arxiv.org/abs/2312.10461&gt;&#13;
* 代码：&lt;https://github.com/chuangchuangtan/NPR-DeepfakeDetection&gt;&#13;
&#13;
![image](https://github.com/lartpang/blog/assets/26847524/0c7f34a2-8f24-458e-84c9-37b8253deb5c)&#13;
&#13;
本文主要研究了基于 CNN 的生成网络中的上采样操作，以实现通用的深度伪造检测。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/CVPR%202024%20-%20Rethinking%20the%20Up-Sampling%20Operations%20in%20CNN-based%20Generative%20Network%20for%20Generalizable%20Deepfake%20Detection.html</guid><pubDate>Wed, 10 Apr 2024 06:22:51 +0000</pubDate></item><item><title>ICLR 2024 - FeatUp - A Model-Agnostic Framework for Features at Any Resolution</title><link>https://lartpang.github.io/blog/post/ICLR%202024%20-%20FeatUp%20-%20A%20Model-Agnostic%20Framework%20for%20Features%20at%20Any%20Resolution.html</link><description># ICLR 2024 | FeatUp: A Model-Agnostic Framework for Features at Any Resolution&#13;
&#13;
![image.png](https://cdn.nlark.com/yuque/0/2024/png/192314/1711687661665-4150b85f-8f4c-4849-aecf-9031439eb241.png#averageHue=%23f7f5f3&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=176&amp;id=u5a79a7c9&amp;originHeight=220&amp;originWidth=727&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=32319&amp;status=done&amp;style=none&amp;taskId=u5ffc5add-597a-4447-8790-5275da0e6fe&amp;title=&amp;width=581.6)&#13;
&#13;
* 论文：[https://arxiv.org/abs/2403.10516](https://arxiv.org/abs/2403.10516)&#13;
* 代码：[https://github.com/mhamilton723/FeatUp](https://github.com/mhamilton723/FeatUp)&#13;
&#13;
![image.png](https://cdn.nlark.com/yuque/0/2024/png/192314/1711687763372-51de525a-0b9f-402c-b70d-431f85752bc6.png#averageHue=%239ec668&amp;clientId=u9ddad46e-4ebe-4&amp;from=paste&amp;height=297&amp;id=uc56f3b36&amp;originHeight=371&amp;originWidth=1042&amp;originalType=binary&amp;ratio=1.25&amp;rotation=0&amp;showTitle=false&amp;size=409622&amp;status=done&amp;style=none&amp;taskId=u6b457815-6715-45e7-80e0-73492fdcc6b&amp;title=&amp;width=833.6)&#13;
&#13;
## 背景动机&#13;
&#13;
深层特征是计算机视觉研究的基石，捕获图像语义并使社区即使在零或少样本情况下也能解决下游任务。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/ICLR%202024%20-%20FeatUp%20-%20A%20Model-Agnostic%20Framework%20for%20Features%20at%20Any%20Resolution.html</guid><pubDate>Wed, 03 Apr 2024 05:02:42 +0000</pubDate></item><item><title>Arixv 2403 - Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey</title><link>https://lartpang.github.io/blog/post/Arixv%202403%20-%20Parameter-Efficient%20Fine-Tuning%20for%20Large%20Models%20A%20Comprehensive%20Survey.html</link><description># Arixv 2403 | Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey&#13;
&#13;
![image.png](https://cdn.nlark.com/yuque/0/2024/png/192314/1711865052255-f9c75ccf-1daf-488d-ae59-fd854eef2301.png#averageHue=%23f0efef&amp;clientId=u6331460c-e958-4&amp;from=paste&amp;height=176&amp;id=u1a3b8c64&amp;originHeight=351&amp;originWidth=1345&amp;originalType=binary&amp;ratio=2&amp;rotation=0&amp;showTitle=false&amp;size=104896&amp;status=done&amp;style=none&amp;taskId=ue922bda5-62cc-4bb2-b22e-d0a375624b0&amp;title=&amp;width=672.5)&#13;
&#13;
* 论文：[https://arxiv.org/abs/2403.14608](https://arxiv.org/abs/2403.14608)&#13;
* 语雀文档：[https://www.yuque.com/lart/papers/gvqrizgggd22g88n](https://www.yuque.com/lart/papers/gvqrizgggd22g88n)&#13;
&#13;
大型模型代表了多个应用领域的突破性进步，在各种任务中取得了显着的成就。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Arixv%202403%20-%20Parameter-Efficient%20Fine-Tuning%20for%20Large%20Models%20A%20Comprehensive%20Survey.html</guid><pubDate>Wed, 03 Apr 2024 04:46:25 +0000</pubDate></item><item><title>Tips for Qt</title><link>https://lartpang.github.io/blog/post/Tips%20for%20Qt.html</link><description>## Set a proper mirror for MaintenanceTool.exe&#13;
&#13;
From: https://mirrors.tuna.tsinghua.edu.cn/help/qt/&#13;
&#13;
```shell&#13;
.\MaintenanceTool.exe --mirror https://mirrors.tuna.tsinghua.edu.cn/qt&#13;
```。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Tips%20for%20Qt.html</guid><pubDate>Mon, 25 Mar 2024 09:49:38 +0000</pubDate></item><item><title>Six methods of indexing pixels of Mat in OpenCV</title><link>https://lartpang.github.io/blog/post/Six%20methods%20of%20indexing%20pixels%20of%20Mat%20in%20OpenCV.html</link><description>## `.at&lt;&gt;()`&#13;
&#13;
```cpp&#13;
// modify the pixel directly&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols; ++w) {&#13;
        image.at&lt;Vec3b&gt;(h, w)[0] = 255;&#13;
        image.at&lt;Vec3b&gt;(h, w)[1] = 0;&#13;
        image.at&lt;Vec3b&gt;(h, w)[2] = 0;&#13;
    }&#13;
}&#13;
&#13;
// modify the pixel by the reference&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols; ++w) {&#13;
        Vec3b&amp; bgr = image.at&lt;Vec3b&gt;(h, w);&#13;
        bgr.val[0] = 0;&#13;
        bgr.val[1] = 255;&#13;
        bgr.val[2] = 0;&#13;
    }&#13;
}&#13;
&#13;
// the image has one channel&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        image.at&lt;uchar&gt;(h, w) = 128;&#13;
    }&#13;
}&#13;
```&#13;
&#13;
## `.ptr&lt;&gt;()`&#13;
&#13;
```cpp&#13;
// use uchar type&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        uchar* ptr = image.ptr&lt;uchar&gt;(h, w);&#13;
        ptr[0] = 255;&#13;
        ptr[1] = 0;&#13;
        ptr[2] = 0;&#13;
    }&#13;
}&#13;
// use cv::Vec3b type&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        Vec3b* ptr = image.ptr&lt;Vec3b&gt;(h, w);&#13;
        ptr-&gt;val[0] = 0;&#13;
        ptr-&gt;val[1] = 255;&#13;
        ptr-&gt;val[2] = 0;&#13;
    }&#13;
}&#13;
&#13;
// use the row pointer and the image has one channel&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    uchar* ptr = image.ptr(h);&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        ptr[w] = 128;&#13;
    }&#13;
}&#13;
&#13;
// use the pixel pointer and the image has one channel&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        uchar* ptr = image.ptr&lt;uchar&gt;(h, w);&#13;
        *ptr = 255;&#13;
    }&#13;
}&#13;
```&#13;
&#13;
## `iterator`&#13;
&#13;
```cpp&#13;
// the image has three channels&#13;
Mat_&lt;Vec3b&gt;::iterator it = image.begin&lt;Vec3b&gt;();&#13;
Mat_&lt;Vec3b&gt;::iterator itend = image.end&lt;Vec3b&gt;();&#13;
for (; it != itend; ++it) {&#13;
    (*it)[0] = 255;&#13;
    (*it)[1] = 0;&#13;
    (*it)[2] = 0;&#13;
}&#13;
&#13;
// the image has one channel&#13;
Mat_&lt;uchar&gt;::iterator it1 = image.begin&lt;uchar&gt;();&#13;
Mat_&lt;uchar&gt;::iterator itend1 = image.end&lt;uchar&gt;();&#13;
for (; it1 != itend1; ++it1) {&#13;
    (*it1) = 128;&#13;
}&#13;
```&#13;
&#13;
## `.data` pointer&#13;
&#13;
```cpp&#13;
// 3 channels&#13;
uchar* data = image.data;&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        *data++ = 128;&#13;
        *data++ = 128;&#13;
        *data++ = 128;&#13;
    }&#13;
}&#13;
&#13;
// 1 channel&#13;
uchar* data = image.data;&#13;
for (int h = 0; h &lt; image.rows; ++h) {&#13;
    for (int w = 0; w &lt; image.cols / 2; ++w) {&#13;
        *data++ = 128;&#13;
    }&#13;
}&#13;
```&#13;
&#13;
## `.row()` and `.col()`&#13;
&#13;
```cpp&#13;
for (int i = 0; i &lt; 100; ++i) {&#13;
    image.row(i).setTo(Scalar(0, 0, 0)); // modify the i th row data&#13;
    image.col(i).setTo(Scalar(0, 0, 0)); // modify the i th column data&#13;
}&#13;
```&#13;
&#13;
## when `isContinuous()` is true&#13;
&#13;
```cpp&#13;
Mat image = imread("...");&#13;
int nRows = image.rows;&#13;
int nCols = image.cols * image.channels();&#13;
&#13;
if (image.isContinuous()) {&#13;
    nCols = nRows * nCols;&#13;
    nRows = 1;&#13;
}&#13;
&#13;
for (int h = 0; h &lt; nRows; ++h) {&#13;
    uchar* ptr = image.ptr&lt;uchar&gt;(h);&#13;
    for (int w = 0; w &lt; nCols; ++w) {&#13;
        // ptr[w] = 128 ;&#13;
        *ptr++ = 128;&#13;
    }&#13;
}&#13;
```&#13;
&#13;
## Reference&#13;
&#13;
- http://t.csdn.cn/bSDNn&#13;
。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Six%20methods%20of%20indexing%20pixels%20of%20Mat%20in%20OpenCV.html</guid><pubDate>Mon, 25 Mar 2024 09:41:52 +0000</pubDate></item><item><title>Snippets of OpenVINO-CPP for Model Inference</title><link>https://lartpang.github.io/blog/post/Snippets%20of%20OpenVINO-CPP%20for%20Model%20Inference.html</link><description>## Header File&#13;
&#13;
```cpp&#13;
#include &lt;openvino/openvino.hpp&gt;&#13;
```&#13;
&#13;
## Create Infer Request&#13;
&#13;
```cpp&#13;
void preprocessing(std::shared_ptr&lt;ov::Model&gt; model) {&#13;
  ov::preprocess::PrePostProcessor ppp(model);&#13;
  ppp.input().tensor().set_layout("NHWC"); // input data is NHWC from OpenCV Mat&#13;
  ppp.input().model().set_layout("NCHW"); // In the model, the layout is NCHW&#13;
  model = ppp.build();&#13;
}&#13;
&#13;
ov::Core core;&#13;
&#13;
auto model = core.read_model(model_path); # can use onnx or openvino's xml file&#13;
preprocessing(model);&#13;
&#13;
auto compiled_model = core.compile_model(model, "CPU");  // Or without `"CPU"`&#13;
auto input_port = compiled_model.input();&#13;
auto infer_request = compiled_model.create_infer_request();&#13;
```&#13;
&#13;
## Input and Output&#13;
&#13;
- single input&#13;
&#13;
```cpp&#13;
infer_request.set_input_tensor(blob);&#13;
infer_request.crop_net.infer();&#13;
```&#13;
&#13;
- single output&#13;
&#13;
```cpp&#13;
ov::Tensor single_output = this-&gt;point_net.get_output_tensor(0);&#13;
```&#13;
&#13;
- multiple outputs&#13;
&#13;
```cpp&#13;
ov::Tensor multi_outputs0 = this-&gt;point_net.get_output_tensor(0);&#13;
ov::Tensor multi_outputs1 = this-&gt;point_net.get_output_tensor(1);&#13;
```&#13;
&#13;
## OpenCV `cv::Mat` &lt;-&gt; OpenVINO `ov::Tensor`&#13;
&#13;
The key to these steps is the alignment of the data layout.&#13;
&#13;
### `cv::Mat` -&gt; `ov::Tensor`&#13;
&#13;
```cpp&#13;
// converting the uint8 3-channels image mat to a float32 tensor&#13;
image.convertTo(image, CV_32FC3, 1.0 / 255);&#13;
// NHWC layout as mentioned above. (N=1, C=3)&#13;
ov::Tensor blob(input_port.get_element_type(), input_port.get_shape(), (float *)image.data);&#13;
```&#13;
&#13;
### `ov::Tensor` -&gt; `cv::Mat`&#13;
&#13;
```cpp&#13;
// tensor follows the NCHW layout, so tensor_shape is (N,C,H,W)&#13;
ov::Shape tensor_shape = tensor.get_shape();&#13;
// Due to N=1 and C=1, we can directly assign all data to a new mat.&#13;
cv::Mat mat(tensor_shape[2], tensor_shape[3], CV_32F, tensor.data());&#13;
```&#13;
&#13;
## Reference&#13;
&#13;
- https://github.com/OpenVINO-dev-contest/YOLOv7_OpenVINO_cpp-python/blob/main/cpp/main_preprocessing.cpp&#13;
- https://github.com/openvinotoolkit/openvino/blob/master/samples/cpp/hello_classification/main.cpp。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Snippets%20of%20OpenVINO-CPP%20for%20Model%20Inference.html</guid><pubDate>Mon, 25 Mar 2024 09:41:05 +0000</pubDate></item><item><title>Build OpenCV and OpenVINO for Windows 10 with VS 2022</title><link>https://lartpang.github.io/blog/post/Build%20OpenCV%20and%20OpenVINO%20for%20Windows%2010%20with%20VS%202022.html</link><description>In this guide, I will build the two powerful open-source libraries, i.e., OpenCV and OpenVINO for running my deeplearning model on windows 10.&#13;
Interestingly, both libraries are closely associated with Intel 🖥️. &#13;
&#13;
## OpenCV 😮 &#13;
&#13;
First of all, we must download the related code projects (`opencv` and `opencv_contrib` containing some plugins for `opencv`) into our computer from this links:&#13;
&#13;
- https://github.com/opencv/opencv/releases&#13;
- https://github.com/opencv/opencv_contrib/tags&#13;
&#13;
Make sure the selected versions of the two libararies are the same.&#13;
Here, I choice the latest version `4.7.0`.&#13;
Because we will recompiling them by ourselves, we can just download the source code zip files.&#13;
Put the two unpacked libraries into the same parent folder `opencv_dir` as follows:&#13;
&#13;
```&#13;
-opencv_dir&#13;
  -opencv-4.7.0&#13;
    -...&#13;
  -opencv_contrib-4.7.0&#13;
    -modules&#13;
    -...&#13;
```&#13;
&#13;
**NOTE**: To avoid the network issue that may be encountered during using CMake, we need to add the url proxy prefix `https://ghproxy.com/` before the urls of some setting of the relevant modules like `https://ghproxy.com/https://raw.github***`:&#13;
- `.cmake` in `opencv-4.7.0/3rdparty/ippicv`&#13;
- `.cmake` in `opencv-4.7.0/3rdparty/ffmpeg`&#13;
- `CMakeLists.txt` in `opencv_contrib-4.7.0/modules/face`&#13;
- Files in `cmake` of `opencv_contrib-4.7.0/modules/xfeatures2d`&#13;
- `CMakeLists.txt` in `opencv_contrib-4.7.0/modules/wechat_qrcode`&#13;
- `CMakeLists.txt` in `opencv_contrib-4.7.0/modules/cudaoptflow`&#13;
&#13;
Next, start compiling OpenCV.&#13;
&#13;
1. Create the build folder: `cd opencv_dir &amp;&amp; mkdir opencv-build-vs2022`&#13;
2. Configure and generate the VS solution by CMake with some config items:&#13;
  - General:&#13;
    - source folder: `&lt;opencv-4.7.0&gt;`&#13;
    - build folder: `&lt;opencv-build-vs2022&gt;`&#13;
    - `BUILD_OPENCV_WORLD=ON`&#13;
    - `CMAKE_BUILD_TYPE=RELEASE`&#13;
    - `OPENCV_ENABLE_NONFREE=ON`&#13;
    - `BUILD_opencv_dnn=ON`&#13;
    - `OPENCV_EXTRA_MODULES_PATH=&lt;opencv_contrib-4.7.0/modules&gt;`&#13;
  - CUDA:&#13;
    - `WITH_CUDA=ON`&#13;
    - `WITH_CUDNN=ON`&#13;
    - `WITH_CUBLAS=ON`&#13;
    - `WITH_CUFFT=ON`&#13;
    - `CUDA_FAST_MATH=ON`&#13;
    - `CUDA_ARCH_BIN=7.5` (We can fill the single value corresponding to the real GPU for accelerating the compilation process.)&#13;
    - `OPENCV_DNN_CUDA=ON`&#13;
3. Go to the build directory: `cd &lt;opencv-build-vs2022&gt;`&#13;
4. Start build by cmake and msvc compiler: `cmake --build . --config Release --verbose -j8`&#13;
5. Install the built opencv into the `install` folder in the current path: `cmake --install . --prefix install`&#13;
6. Add the `bin` directory into the user environment: `&lt;path&gt;\install\x64\vc17\bin`&#13;
7. In VS:&#13;
    - add the `&lt;path&gt;\install\include` directory into "解决方案资源管理器-&gt;右键点击属性-&gt;VC++目录-&gt;外部包含目录"&#13;
    - add the `&lt;path&gt;\install\x64\vc17\lib` directory into "解决方案资源管理器-&gt;右键点击属性-&gt;VC++目录-&gt;库目录"&#13;
    - add the `opencv_world470.lib` into "解决方案资源管理器-&gt;右键点击属性-&gt;链接器-&gt;输入-&gt;附加依赖项"&#13;
&#13;
## OpenVINO 🍰 &#13;
&#13;
The document of OpenVINO is intuitive and the readability is better than OpenCV.&#13;
The relevant content about building and installing the libirary is listed in these links:&#13;
- https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/build_windows.md&#13;
- https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/cmake_options_for_custom_comiplation.md&#13;
- https://github.com/openvinotoolkit/openvino/blob/master/docs/dev/installing.md&#13;
&#13;
After building and install the OpenCV library, it's time to move on to OpenVINO.&#13;
&#13;
1. We need clone the project and the sub modules.&#13;
    ```&#13;
    git clone https://github.com/openvinotoolkit/openvino.git&#13;
    cd openvino&#13;
    git submodule update --init --recursive&#13;
    ```&#13;
2. Create the build folder: `mkdir build &amp;&amp; cd build`&#13;
3. Configure and generate the VS solution by CMake:&#13;
    - `ENABLE_INTEL_GPU=OFF` (We only use the Intel CPU.)&#13;
    - Disable some frontend items:&#13;
      - `ENABLE_OV_PDPD_FRONTEND=OFF`&#13;
      - `ENABLE_OV_TF_FRONTEND=OFF`&#13;
      - `ENABLE_OV_TF_LITE_FRONTEND=OFF`&#13;
      - `ENABLE_OV_PYTORCH_FRONTEND=OFF`&#13;
    - For Python:&#13;
      - `ENABLE_PYTHON=ON` It seems that `openvino-dev` needs to be installed first in the detected environment, otherwise a warning message will be thrown in the cmake-gui window.&#13;
      - `PYTHON_EXECUTABLE=&lt;python.exe&gt;`&#13;
      - `PYTHON_INCLUDE_DIR=&lt;incude directory&gt;`&#13;
      - `PYTHON_LIBIRARY=&lt;pythonxx.lib in libs directory&gt;`&#13;
    - For OpenCV:&#13;
      - `ENABLE_OPENCV=ON`&#13;
      - `OpenCV_DIR=&lt;opencv-build-vs2022/install&gt;`&#13;
4. Build the library: `cmake --build . --config Release --verbose -j8`&#13;
5. Install the library into the `install` directory: `cmake --install . --prefix install`&#13;
6. Add the `bin` directory into the environment:&#13;
    - `&lt;path&gt;\install\runtime\bin\intel64\Release`&#13;
    - `&lt;path&gt;\install\runtime\3rdparty\tbb\bin`&#13;
8. In VS:&#13;
    - add the `&lt;path&gt;\install\runtime\include` directory into "解决方案资源管理器-&gt;右键点击属性-&gt;VC++目录-&gt;外部包含目录"&#13;
    - add the `&lt;path&gt;\install\runtime\lib\intel64\Release` directory into "解决方案资源管理器-&gt;右键点击属性-&gt;VC++目录-&gt;库目录"&#13;
    - add the 🌟 `openvino.lib`, 🌟 `openvino_onnx_frontend.lib`, `openvino_c.lib` into "解决方案资源管理器-&gt;右键点击属性-&gt;链接器-&gt;输入-&gt;附加依赖项"&#13;
&#13;
## Set DLL path in IDE&#13;
&#13;
- VS: "right click on solution -&gt; Properties -&gt; Debugging -&gt; Environment -&gt; `PATH=&lt;path&gt;\install\x64\vc17\bin;%PATH%`"&#13;
- Qt Creator: "Projects -&gt; Build &amp; Run -&gt; Build/Run -&gt; Environment -&gt; Details -&gt; Eidt %PATH% -&gt; Add `&lt;path&gt;\install\x64\vc17\bin`"。</description><guid isPermaLink="true">https://lartpang.github.io/blog/post/Build%20OpenCV%20and%20OpenVINO%20for%20Windows%2010%20with%20VS%202022.html</guid><pubDate>Mon, 25 Mar 2024 09:38:03 +0000</pubDate></item></channel></rss>